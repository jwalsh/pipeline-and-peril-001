#+TITLE: Literate Programming & Game Balance Through Simulation
#+SUBTITLE: A Case Study with Pipeline & Peril
#+AUTHOR: Jason Walsh
#+DATE: 2025-09-10
#+OPTIONS: toc:t num:t ^:nil
#+REVEAL_ROOT: https://cdn.jsdelivr.net/npm/reveal.js
#+REVEAL_THEME: moon
#+REVEAL_TRANS: slide
#+REVEAL_PLUGINS: (markdown notes highlight zoom)

* Introduction
:PROPERTIES:
:reveal_background: #2E3440
:END:

** The Challenge

#+ATTR_REVEAL: :frag (appear)
- Board games require extensive playtesting
- Balance is critical but time-consuming to achieve
- Edge cases are hard to discover manually
- Statistical validation needs thousands of games

** Our Solution

#+BEGIN_QUOTE
Combine *literate programming*, *automated simulation*, and *scientific experimentation* to rapidly iterate on game design
#+END_QUOTE

#+ATTR_REVEAL: :frag roll-in
🎮 + 🐍 + 📊 = 🚀

* Part I: Literate Programming
:PROPERTIES:
:reveal_background: #3B4252
:END:

** What is Literate Programming?

#+BEGIN_SRC org
,#+TITLE: Pipeline & Peril Requirements
,#+PROPERTY: header-args :mkdirp yes

,* Overview
This document contains all requirements...

,* Implementation
,#+BEGIN_SRC python :tangle src/game_engine.py
def simulate_game(players):
    """Core game simulation logic"""
    # Implementation here
,#+END_SRC
#+END_SRC

** Benefits for Game Development

#+ATTR_REVEAL: :frag (appear)
1. *Documentation and code stay synchronized*
2. *Requirements directly generate implementation*
3. *Tests emerge from specifications*
4. *Single source of truth*

** Our Implementation

#+BEGIN_SRC bash
# Extract all code from literate document
emacs --batch --eval "(progn 
  (find-file \"pipeline-peril-pygame-literate.org\")
  (org-babel-tangle))"

# Result: Complete project structure
#+END_SRC

Generated:
- 5 markdown documents
- 3 Python modules
- 2 shell scripts
- Complete test suite

* Part II: Simulation Architecture
:PROPERTIES:
:reveal_background: #434C5E
:END:

** Core Components

#+BEGIN_SRC python
@dataclass
class GameState:
    """Complete game state representation"""
    players: List[Player]
    services: Dict[str, Service]
    grid: HexGrid
    metrics: GameMetrics
    
    def simulate_round(self) -> None:
        """Execute one complete game round"""
        self.traffic_phase()
        self.action_phase()
        self.resolution_phase()
        self.chaos_phase()
#+END_SRC

** Autonomous Agents

#+ATTR_REVEAL: :frag (appear)
- *Aggressive*: Maximize service count
- *Defensive*: Focus on redundancy
- *Balanced*: Mixed strategy
- *Adaptive*: Learn from game state
- *Random*: Baseline comparison

** Performance Optimization

#+BEGIN_SRC python
# Concurrent simulation with asyncio
async def run_batch(games: int) -> List[GameResult]:
    tasks = [simulate_game_async() for _ in range(games)]
    return await asyncio.gather(*tasks)

# Result: 1000 games in < 60 seconds
#+END_SRC

* Part III: Experimental Framework
:PROPERTIES:
:reveal_background: #4C566A
:END:

** Scientific Method for Games

#+ATTR_REVEAL: :frag (appear)
1. *Hypothesis*: "Reducing service costs increases game length"
2. *Variables*: Service costs, game duration
3. *Control*: Grid size, player count, strategies
4. *Measurement*: Rounds to completion, winner distribution
5. *Analysis*: Statistical significance testing

** Experiment Structure

#+BEGIN_SRC text
experiments/
├── 001-service-costs/
│   ├── Makefile         # Automation
│   ├── README.md        # Hypothesis & method
│   ├── inputs/          # Parameter variations
│   ├── outputs/         # Raw results
│   └── analysis.ipynb   # Statistical analysis
#+END_SRC

** Automation Pipeline

#+BEGIN_SRC makefile
# Makefile for each experiment
.PHONY: all clean run analyze

all: outputs/results.json outputs/report.html

outputs/results.json: inputs/*.yaml
	python run_experiment.py --config inputs/ \
	  --output outputs/results.json

outputs/report.html: outputs/results.json
	jupyter nbconvert --execute analysis.ipynb \
	  --to html --output outputs/report.html
#+END_SRC

* Part IV: Five Key Experiments
:PROPERTIES:
:reveal_background: #5E81AC
:END:

** Experiment 001: Service Cost Optimization

#+ATTR_REVEAL: :frag (appear)
- *Question*: What service costs create balanced gameplay?
- *Method*: Vary costs ±50%, measure game length and win rates
- *Result*: Sweet spot at current costs ±15%
- *Impact*: Validated design decisions

** Experiment 002: Grid Size Impact

#+ATTR_REVEAL: :frag (appear)
- *Question*: How does board size affect strategy?
- *Method*: Test 6×6, 8×6, 10×8, 12×10 grids
- *Result*: 8×6 optimal for 3-4 players
- *Impact*: Confirmed physical board dimensions

** Experiment 003: Chaos Frequency Tuning

#+ATTR_REVEAL: :frag (appear)
- *Question*: How often should chaos events occur?
- *Method*: Vary entropy thresholds and event probability
- *Result*: Events every 3-4 rounds maintain tension
- *Impact*: Refined chaos mechanics

** Experiment 004: Victory Conditions

| Condition | Games | Avg Rounds | Satisfaction |
|-----------|-------|------------|--------------|
| Coop 80%  | 1000  | 12.3       | High         |
| Coop 85%  | 1000  | 15.7       | Medium       |
| Coop 90%  | 1000  | 19.2       | Low          |
| Last Stand| 1000  | 8.9        | High         |

** Experiment 005: AI Strategy Comparison

#+BEGIN_SRC python
results = {
    "aggressive": {"wins": 342, "avg_round": 11.2},
    "defensive":  {"wins": 289, "avg_round": 14.5},
    "balanced":   {"wins": 369, "avg_round": 12.8},
}
# Balanced strategy shows slight advantage
#+END_SRC

* Part V: Results & Insights
:PROPERTIES:
:reveal_background: #88C0D0
:END:

** Statistical Validation

#+ATTR_REVEAL: :frag (appear)
- ✅ 10,000+ games simulated
- ✅ p < 0.05 for all major findings
- ✅ Reproducible results
- ✅ Edge cases discovered and fixed

** Game Balance Achievements

#+BEGIN_SRC python
# Measured across 5000 games
metrics = {
    "game_length": "12.4 ± 2.1 rounds",     # Target: 10-15
    "coop_win_rate": "23%",                 # Target: 20-30%
    "player_balance": "σ² = 0.03",          # Low variance
    "comeback_possible": "87% of games",     # High drama
}
#+END_SRC

** Discovered Optimizations

#+ATTR_REVEAL: :frag (appear)
1. Cache service: Reduced from 8→6 capacity
2. Chaos trigger: Changed from entropy >5 to >3
3. Starting resources: Increased from 3→5 each
4. Victory threshold: Lowered from 85% to 80%

* Part VI: Publication-Ready Artifacts
:PROPERTIES:
:reveal_background: #8FBCBB
:END:

** Generated Documentation

#+ATTR_REVEAL: :frag (appear)
- 📚 Comprehensive README with badges
- 📊 Statistical analysis notebooks
- 🎨 Rich terminal visualizations
- 📈 Performance benchmarks
- 🧪 Test coverage reports

** Reproducible Research

#+BEGIN_SRC bash
# Anyone can reproduce our findings
git clone https://github.com/jwalsh/pipeline-and-peril-001
cd pipeline-and-peril-001
make experiments  # Run all experiments
make report       # Generate analysis
#+END_SRC

** Code Quality Metrics

| Metric | Value | Target |
|--------|-------|--------|
| Test Coverage | 94% | >90% |
| Type Coverage | 100% | 100% |
| Docstring Coverage | 87% | >80% |
| Cyclomatic Complexity | 3.2 | <5 |
| Performance | 1000 games/min | >500 |

* Part VII: Lessons Learned
:PROPERTIES:
:reveal_background: #BF616A
:END:

** Literate Programming Benefits

#+ATTR_REVEAL: :frag (appear)
- 🎯 Requirements → Implementation seamlessly
- 📝 Documentation never gets stale
- 🔄 Changes propagate automatically
- 👥 Onboarding is self-contained

** Simulation Insights

#+ATTR_REVEAL: :frag (appear)
- 🎲 10,000 games > 100 playtest sessions
- 🐛 Found edge cases humans miss
- 📊 Statistical confidence in balance
- ⚡ Rapid iteration on mechanics

** Modern Python Power

#+BEGIN_SRC python
# Pattern matching for game logic
match game.phase:
    case "traffic": handle_traffic()
    case "action": handle_actions()
    case "chaos" if entropy > 3: trigger_chaos()

# Rich console for beautiful output
with Live(layout, refresh_per_second=10) as live:
    simulate_with_visualization()
#+END_SRC

* Conclusion
:PROPERTIES:
:reveal_background: #2E3440
:END:

** Key Takeaways

#+ATTR_REVEAL: :frag (appear)
1. *Literate programming* unifies design and implementation
2. *Simulation* provides statistical validation
3. *Experiments* reveal optimal parameters
4. *Automation* ensures reproducibility
5. *Modern tools* enhance developer experience

** The Future

#+ATTR_REVEAL: :frag (appear)
- 🤖 Machine learning for strategy optimization
- 🌐 Web-based playtesting platform
- 📱 Mobile companion apps
- 🎮 Real-time multiplayer
- 📊 Advanced analytics dashboard

** Thank You!

#+BEGIN_CENTER
🎮 *Pipeline & Peril* 🎮

A demonstration of modern game development practices

[[https://github.com/jwalsh/pipeline-and-peril-001][github.com/jwalsh/pipeline-and-peril-001]]

Questions?
#+END_CENTER

* Appendix: Technical Details
:PROPERTIES:
:reveal_background: #3B4252
:END:

** Technology Stack

- Python 3.13 with pattern matching
- Pydantic v2 for validation
- Rich for terminal UI
- uv for package management
- Pytest for testing
- Jupyter for analysis

** Performance Benchmarks

| Operation | Time | Memory |
|-----------|------|--------|
| Single game | 48ms | 12MB |
| 1000 games | 52s | 145MB |
| Analysis | 3.2s | 89MB |
| Report generation | 8.1s | 210MB |

** Statistical Methods

- Monte Carlo simulation
- Hypothesis testing (t-tests)
- ANOVA for multi-factor analysis
- Regression for parameter optimization
- Bootstrap for confidence intervals